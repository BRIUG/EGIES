#The root directory of the path in this configuration file, root directory + other paths = absolute path
#Windows users please note that the path separator is used uniformly /
# For more information, please go to http://hanlp.linrunsoft.com/
root=Q:/hanlp/
#Core dictionary path
CoreDictionaryPath=data/dictionary/CoreNatureDictionary.txt
#2-yuan grammar dictionary path
BiGramDictionaryPath=data/dictionary/CoreNatureDictionary.ngram.txt
#Stop word dictionary path
CoreStopWordDictionaryPath=data/dictionary/stopwords.txt
#Synonym dictionary path
CoreSynonymDictionaryDictionaryPath=data/dictionary/synonym/CoreSynonym.txt
#Name dictionary path
PersonDictionaryPath=data/dictionary/person/nr.txt
#Name dictionary transfer matrix path
PersonDictionaryTrPath=data/dictionary/person/nr.tr.txt
#Traditional dictionary root directory
tcDictionaryRoot=data/dictionary/tc
#Custom dictionary path, use; separate multiple custom dictionary, the beginning of the space indicates the same directory, use the "file name" form to indicate that the dictionary's part of speech default is the part of speech. The priority is decremented.
#data/dictionary/custom/CustomDictionary.is a high quality thesaurus, please don't delete it. All dictionaries use UTF-8 encoding uniformly.
CustomDictionaryPath=data/dictionary/custom/CustomDictionary.txt; 现代汉语补充词库.txt; 全国地名大全.txt GlobalPlaces.txt ns; 人名词典.txt; 机构名词典.txt; 上海地名.txt ns;data/dictionary/person/nrf.txt nrf;
#CRF word segmentation model path
CRFSegmentModelPath=data/model/segment/CRFSegmentModel.txt
#HMM participle model
HMMSegmentModelPath=data/model/segment/HMMSegmentModel.bin
#Whether the word segmentation results show part of speech
ShowTermNature=true
#IO adapter that implements the com.hankcs.hanlp.corpus.io.IIOAdapter interface to run HanLP on different platforms (Hadoop, Redis, etc.)
#The default IO adapter is as follows, the adapter is based on the normal file system.
#IOAdapter=com.hankcs.hanlp.corpus.io.FileIOAdapter